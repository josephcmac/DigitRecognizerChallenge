{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 00:36:30.941698: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-10 00:36:30.941723: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, StructField, StructType\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import convert_to_tensor\n",
    "from tensorflow import reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 9 #digit to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/10 00:36:35 WARN Utils: Your hostname, Caballero resolves to a loopback address: 127.0.1.1; using 10.0.0.15 instead (on interface wlp0s20f3)\n",
      "21/12/10 00:36:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/caballero/anaconda3/envs/py37/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/12/10 00:36:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/10 00:36:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/10 00:36:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/12/10 00:36:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('train.csv')\n",
    "L = array([list(train.asDict().values()) for train in df.collect()])\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = L[:,1:785]\n",
    "y_train = L[:,0]\n",
    "del(L)\n",
    "x_train = [x/255. for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 00:36:51.679490: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-10 00:36:51.679514: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-10 00:36:51.679526: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Caballero): /proc/driver/nvidia/version does not exist\n",
      "2021-12-10 00:36:51.679872: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-10 00:36:51.680581: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 131712000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "x_train = convert_to_tensor(x_train, dtype=\"float32\")\n",
    "x_train = reshape(x_train, (x_train.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use binary classifiers, where modelD is 1 iff the input is d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4200/4200 [==============================] - 51s 12ms/step - loss: 0.0523 - accuracy: 0.9829\n",
      "Epoch 2/10\n",
      "4200/4200 [==============================] - 51s 12ms/step - loss: 0.0224 - accuracy: 0.9933\n",
      "Epoch 3/10\n",
      "4200/4200 [==============================] - 53s 13ms/step - loss: 0.0172 - accuracy: 0.9943\n",
      "Epoch 4/10\n",
      "4200/4200 [==============================] - 53s 13ms/step - loss: 0.0156 - accuracy: 0.9950\n",
      "Epoch 5/10\n",
      "4200/4200 [==============================] - 51s 12ms/step - loss: 0.0110 - accuracy: 0.9963\n",
      "Epoch 6/10\n",
      "4200/4200 [==============================] - 51s 12ms/step - loss: 0.0115 - accuracy: 0.9964\n",
      "Epoch 7/10\n",
      "4200/4200 [==============================] - 51s 12ms/step - loss: 0.0094 - accuracy: 0.9970\n",
      "Epoch 8/10\n",
      "4200/4200 [==============================] - 51s 12ms/step - loss: 0.0094 - accuracy: 0.9971\n",
      "Epoch 9/10\n",
      "4200/4200 [==============================] - 52s 12ms/step - loss: 0.0070 - accuracy: 0.9978\n",
      "Epoch 10/10\n",
      "4200/4200 [==============================] - 47s 11ms/step - loss: 0.0096 - accuracy: 0.9973\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "y_trainD = [ to_categorical( 1 if y == d else 0 , 2) for y in y_train ]\n",
    "y_trainD = convert_to_tensor(y_trainD, dtype=\"float32\")\n",
    "model.fit(x_train, y_trainD, batch_size=10, epochs=10, shuffle=True)\n",
    "del(y_trainD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(x_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv('test.csv')\n",
    "x_pred = array([list(train.asDict().values()) for train in df.collect()])\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = [x/255. for x in x_pred]\n",
    "x_pred = convert_to_tensor(x_pred, dtype=\"float32\")\n",
    "x_pred = reshape(x_pred, (x_pred.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_pred, batch_size=10)\n",
    "del(x_pred)\n",
    "y_pred = [ y_pred[j][1] for j in range( len(y_pred) ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [(j+1, float(y_pred[j])) for j in range(len(y_pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = spark.createDataFrame(spark.sparkContext.parallelize(y_pred),  StructType([\n",
    "    StructField('ImageId', IntegerType(), True),\n",
    "    StructField('Probability', FloatType(), True)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.toPandas().to_csv('component'+str(d)+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "609d861bae671264e9878e9f2f5a015669cd6b7c553d269671411b0c8e33f9ae"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
